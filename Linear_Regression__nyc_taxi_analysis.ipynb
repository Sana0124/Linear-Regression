{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import Library"
      ],
      "metadata": {
        "id": "DUFu83jC-fwa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, roc_curve\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import KFold\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "import warnings\n",
        "from sklearn.exceptions import ConvergenceWarning\n",
        "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
        "import numpy as np\n",
        "from sklearn.linear_model import Ridge, LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error"
      ],
      "metadata": {
        "id": "_NFUn1sbRv_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Read Data From Google Drive"
      ],
      "metadata": {
        "id": "7mVNM4yo-kNt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "id": "_pjbWheI_hyQ",
        "outputId": "859f3dc1-5321-4e11-add1-a891d6fbfeff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-a145c0899d7d>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    130\u001b[0m   )\n\u001b[1;32m    131\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this assignment, you are tasked with predicting the fare amount (inclusive of tolls) for a taxi ride in New York City given the pickup and dropoff locations. You can get a basic estimate based on just the distance between the two points, this will result in an RMSE of $5-$8.\n",
        "\n",
        "This is a large dataset something which is not covered yet in the class  but feel free to to look at the various techniques used to handle it.\n",
        "\n"
      ],
      "metadata": {
        "id": "1B0xncBfQGqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "   **Task**: Load the first 100,000 rows of data from the `train.csv` file located in the taxi dataset link on your uplevel. Skip the first row and use the second row as the header. Store the data in a variable named 'data'."
      ],
      "metadata": {
        "id": "28AW5sd5GGQI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace the file_path with the path to your train.csv file\n",
        "file_path = \"path/to/your/train.csv\"\n",
        "data = pd.read_csv(file_path, header=0, nrows=100000)"
      ],
      "metadata": {
        "id": "aQk2tjvkRHcJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EDA & Feature Engineering"
      ],
      "metadata": {
        "id": "ocwm_2Tg-sc1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary statistics and missing values"
      ],
      "metadata": {
        "id": "Ub6hD-V3UA1q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.describe()"
      ],
      "metadata": {
        "id": "JT0j7WlKUCye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for missing values\n",
        "data.isnull().sum()"
      ],
      "metadata": {
        "id": "4kp-6F9yUS1k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop the rows with missing values\n",
        "data.dropna(inplace=True)"
      ],
      "metadata": {
        "id": "LXwHOa0QUfaH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop the rows with missing values\n",
        "len(data)"
      ],
      "metadata": {
        "id": "xHs9UHaEUiJK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for missing values\n",
        "data.isnull().sum()"
      ],
      "metadata": {
        "id": "DTh3HLLJUzXX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remove value where fare < 0\n",
        "data = data[data[\"fare_amount\"] > 0]"
      ],
      "metadata": {
        "id": "q3SMAjtsU1Hq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(data)"
      ],
      "metadata": {
        "id": "vlA9R14OVByY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task**: Convert the `pickup_datetime` column in the 'data' DataFrame to datetime type. Then, extract the year, month, day, and hour from the `pickup_datetime` column and create new columns named 'year', 'month', 'day', and 'hour' in the DataFrame"
      ],
      "metadata": {
        "id": "rIoSWli-JmxK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the pickup_datetime column to datetime type\n",
        "data['pickup_datetime'] = pd.to_datetime(data['pickup_datetime'])\n",
        "\n",
        "# Extract year, month, day, and hour from the pickup_datetime column\n",
        "data['year'] =  ## TODO - write your solution here\n",
        "data['month'] =  ## TODO - write your solution here\n",
        "data['day'] =  ## TODO - write your solution here\n",
        "data['hour'] =  ## TODO - write your solution here"
      ],
      "metadata": {
        "id": "jrn-vjq5VMhy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Distance Calculation"
      ],
      "metadata": {
        "id": "0PzsQiu2Vg5B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will calculate distance with Haversine Formula\n",
        "\n",
        "Pros of Haversine Formula:\n",
        "1. Accuracy: The Haversine formula is more accurate for calculating distances between two points on the Earth's surface, as it takes into account the Earth's curvature.\n",
        "2. Applicability: The Haversine formula is specifically designed for calculating great-circle distances on a sphere, making it suitable for geospatial applications, such as navigation and geodesy.\n",
        "3. Consistent results: The Haversine formula provides consistent results for distances on a global scale, whereas the Euclidean distance can produce significant errors when used for long distances.\n",
        "\n",
        "Cons of Haversine Formula:\n",
        "1. Complexity: The Haversine formula is more complex than the Euclidean distance, requiring trigonometric functions and additional calculations.\n",
        "2. Performance: Due to its complexity, the Haversine formula can be slower to compute than the Euclidean distance, especially when dealing with large datasets.\n",
        "3. Assumptions: The Haversine formula assumes a perfect sphere, while the Earth is more of an oblate spheroid. This can lead to some inaccuracies, especially at high latitudes."
      ],
      "metadata": {
        "id": "SMe9nCnGV2cC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install haversine"
      ],
      "metadata": {
        "id": "a1MBW2GQWQiN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the distance between pickup and dropoff coordinates using haversine formula\n",
        "from haversine import haversine"
      ],
      "metadata": {
        "id": "wQYZJ42IWKB8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task**: Write a function named `calculate_distance` that takes a row from the DataFrame as input. The function should calculate the distance between the pickup and dropoff coordinates using the haversine formula. The input row has the following columns: 'pickup_latitude', 'pickup_longitude', 'dropoff_latitude', and 'dropoff_longitude'. Return the calculated distance."
      ],
      "metadata": {
        "id": "ljaQXUp4Lq5V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " def calculate_distance(row):\n",
        "       # TODO - Write your solution here\n",
        "       pass\n",
        "       # return haversine(pickup, dropoff)"
      ],
      "metadata": {
        "id": "1yuFHjn3WEB1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['distance'] = data.apply(calculate_distance, axis=1)\n",
        "\n",
        "# Drop the key and pickup_datetime columns as they are not needed for EDA\n",
        "data.drop(['key', 'pickup_datetime'], axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "C6zgTSdeWdpL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Time for some Geography\n",
        "\n",
        "Latitude and longitude values are used to represent geographical coordinates on the Earth's surface. They are measured in degrees and have specific ranges that correspond to valid locations on Earth.\n",
        "\n",
        "Latitude values range from -90 to +90 degrees, where -90 represents the South Pole, 0 represents the Equator, and +90 represents the North Pole.\n",
        "\n",
        "Longitude values range from -180 to +180 degrees, where -180 corresponds to the Prime Meridian (which passes through Greenwich, London), and +180 corresponds to the 180th meridian, which is on the opposite side of the Earth.\n",
        "\n",
        "These ranges are based on the Earth's geometry and are used to ensure that latitude and longitude values correspond to actual locations on the Earth's surface. If a latitude or longitude value is outside its respective range, it is considered invalid and most likely represents an error in the data."
      ],
      "metadata": {
        "id": "T7RWM-hFX6f0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter out rows with invalid latitude and longitude values\n",
        "valid_latitude_range = (-90, 90)\n",
        "valid_longitude_range = (-180, 180)\n",
        "\n",
        "data = data[(data['pickup_latitude'].between(*valid_latitude_range)) &\n",
        "            (data['pickup_longitude'].between(*valid_longitude_range)) &\n",
        "            (data['dropoff_latitude'].between(*valid_latitude_range)) &\n",
        "            (data['dropoff_longitude'].between(*valid_longitude_range))]"
      ],
      "metadata": {
        "id": "HVXL8HMAYFYR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(data)"
      ],
      "metadata": {
        "id": "aPLjIL3oYK_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#let's rerun earlier code\n",
        "\n",
        "data['distance'] = data.apply(calculate_distance, axis=1)\n",
        "\n",
        "# Drop the key and pickup_datetime columns as they are not needed for EDA\n",
        "data.drop(['key', 'pickup_datetime'], axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "7NpvQ-_vYNw6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "   **Task**: Create a histogram plot to visualize the distribution of the `fare_amount` column in the 'data' DataFrame. Use Seaborn's `histplot` function with kernel density estimation (kde) enabled. Add a title \"Distribution of Fare Amount\" and an x-axis label \"Fare Amount\". Display the plot using `plt.show()`."
      ],
      "metadata": {
        "id": "uCE_-mCbMI5t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the distribution of fare_amount\n",
        "# TODO - Write your solution here\n"
      ],
      "metadata": {
        "id": "sUBuEocIYXWA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[:10]"
      ],
      "metadata": {
        "id": "a1o4jG-HYlt6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task**: Create a count plot to visualize the distribution of the `passenger_count` column in the 'data' DataFrame. Use Seaborn's `countplot` function. Add a title \"Distribution of Passenger Count\", an x-axis label \"Passenger Count\", and a y-axis label \"Frequency\". Display the plot using `plt.show()`.|\n"
      ],
      "metadata": {
        "id": "xsZmnuw9W9dY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the distribution of passenger_count\n",
        "# TODO - Write your solution here"
      ],
      "metadata": {
        "id": "DWZV9FBHYfWT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task**: Create a histogram plot to visualize the distribution of the `distance` column in the 'data' DataFrame. Use Seaborn's `histplot` function with kernel density estimation (kde) enabled. Add a title \"Distribution of Distance\" and an x-axis label \"Distance\". Display the plot using `plt.show()`."
      ],
      "metadata": {
        "id": "6qoKEwQFXZdx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the distribution of distance\n",
        "# TODO - Write your solution here"
      ],
      "metadata": {
        "id": "0tY4Mo4PYti7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task**: Create a heatmap to visualize the correlations between the features in the 'data' DataFrame. Use Seaborn's `heatmap` function with annotations enabled and a \"coolwarm\" colormap. Add a title \"Correlation Heatmap\". Display the plot using `plt.show()`."
      ],
      "metadata": {
        "id": "uNcp3SLPXtsN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the heatmap of correlations between features\n",
        "# TODO - Write your solution here"
      ],
      "metadata": {
        "id": "JCcoJm5EZWWE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task**: Create a line plot to visualize the average `fare_amount` by hour in the 'data' DataFrame. Calculate the average fare amount for each hour using the `groupby` method. Use Seaborn's `lineplot` function to create the plot. Add a title \"Average Fare Amount by Hour\", an x-axis label \"Hour\", and a y-axis label \"Average Fare Amount\". Display the plot using `plt.show()`."
      ],
      "metadata": {
        "id": "Lne4YSXgYSQ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the average fare_amount by hour\n",
        "# TODO - Write your solution here"
      ],
      "metadata": {
        "id": "ezEvWorLZjsW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task**: Create a line plot to visualize the average `fare_amount` by month in the 'data' DataFrame. Calculate the average fare amount for each month using the `groupby` method. Use Seaborn's `lineplot` function to create the plot. Add a title \"Average Fare Amount by Month\", an x-axis label \"Month\", and a y-axis label \"Average Fare Amount\". Display the plot using `plt.show()`."
      ],
      "metadata": {
        "id": "192xuEFOYlKo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the average fare_amount by month\n",
        "# TODO - Write your solution here"
      ],
      "metadata": {
        "id": "uuTMthS2ZnhQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task**: Create a line plot to visualize the average `fare_amount` by year in the 'data' DataFrame. Calculate the average fare amount for each year using the `groupby` method. Use Seaborn's `lineplot` function to create the plot. Add a title \"Average Fare Amount by Year\", an x-axis label \"Year\", and a y-axis label \"Average Fare Amount\". Display the plot using `plt.show()`."
      ],
      "metadata": {
        "id": "9WEh3NgDaAI8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the average fare_amount by month\n",
        "# TODO - Write your solution here"
      ],
      "metadata": {
        "id": "AonN1oC9Zrr5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task**: Remove outliers from the 'data' DataFrame, assuming data has already been cleaned for missing values and invalid coordinates. Apply the following conditions:\n",
        "- Keep rows with 'fare_amount' between 1 and 200 (inclusive).\n",
        "- Keep rows with 'distance' between 0.1 and 100 (inclusive).\n",
        "- Keep rows with 'passenger_count' between 1 and 6 (inclusive)."
      ],
      "metadata": {
        "id": "huf8Bb_ZaNH7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove outliers (assuming data has already been cleaned for missing values and invalid coordinates)\n",
        "data = # TODO - Write your solution here"
      ],
      "metadata": {
        "id": "UVEHF7fnktY5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(data)"
      ],
      "metadata": {
        "id": "b5uoRFFyk3tl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train Test split"
      ],
      "metadata": {
        "id": "5CNlfz2NQZtW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task**:\n",
        "\n",
        "a. Define the features (X) by dropping the 'fare_amount', 'pickup_longitude', 'pickup_latitude', 'dropoff_longitude', and 'dropoff_latitude' columns from the 'data' DataFrame.\n",
        "\n",
        "b. Scale the features using the `StandardScaler` from the `sklearn.preprocessing` module. Store the scaled features in a variable named `X_scaled`.\n",
        "\n",
        "c. Define the target variable (y) as the 'fare_amount' column from the 'data' DataFrame.\n",
        "\n",
        "d. Split the dataset into training and testing sets using the `train_test_split` function from the `sklearn.model_selection` module. Use an 80-20 split with a random state of 42. Store the split data in the variables `X_train`, `X_test`, `y_train`, and `y_test`."
      ],
      "metadata": {
        "id": "RUNH71O3N18N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the features and target variable\n",
        "# a. Define the features (X)\n",
        "# X = data.drop([...], axis=1)\n",
        "\n",
        "# b. Scale the features\n",
        "# scaler = StandardScaler()\n",
        "# X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# c. Define the target variable (y)\n",
        "# y = data['fare_amount']\n",
        "\n",
        "# d. Split the data into training and testing sets\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "TTfek43gbJ__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Training"
      ],
      "metadata": {
        "id": "JnpbyYnZ-1zw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task**:\n",
        "\n",
        "a. Train a Ridge Regression model with an alpha of 1.0 using the training data (X_train and y_train). Store the trained model in a variable named `ridge_model`.\n",
        "\n",
        "b. Train a Linear Regression model using the training data (X_train and y_train). Store the trained model in a variable named `linear_model`."
      ],
      "metadata": {
        "id": "Pv-yJbitOdqR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the Ridge Regression model\n",
        "ridge_model = Ridge(alpha=1.0)\n",
        "ridge_model.fit(X_train, y_train)\n",
        "\n",
        "# Train the Linear Regression model\n",
        "linear_model = LinearRegression()\n",
        "linear_model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "p3ftKLKlbsPx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "a. Use the trained Ridge Regression model to make predictions on the testing data (X_test). Store the predictions in a variable named `ridge_preds`.\n",
        "\n",
        "b. Use the trained Linear Regression model to make predictions on the testing data (X_test). Store the predictions in a variable named `linear_preds`."
      ],
      "metadata": {
        "id": "_Oxd8aq2O0Er"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions using the Ridge Regression model\n",
        "ridge_preds = ridge_model.predict(X_test)\n",
        "\n",
        "# Make predictions using the Linear Regression model\n",
        "linear_preds = linear_model.predict(X_test)"
      ],
      "metadata": {
        "id": "Jq9TyQTfceiD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task**:\n",
        "\n",
        "a. Calculate the Root Mean Squared Error (RMSE) for the Ridge Regression model using the true values (y_test) and the predicted values (ridge_preds). Store the result in a variable named `ridge_rmse` and print it.\n",
        "\n",
        "b. Calculate the Root Mean Squared Error (RMSE) for the Linear Regression model using the true values (y_test) and the predicted values (linear_preds). Store the result in a variable named `linear_rmse` and print it.\n"
      ],
      "metadata": {
        "id": "iXEL-QtPPV-o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the RMSE for the Ridge Regression model\n",
        "ridge_rmse = np.sqrt(mean_squared_error(y_test, ridge_preds))\n",
        "print(f\"Ridge Regression RMSE: {ridge_rmse}\")\n",
        "\n",
        "# Calculate the RMSE for the Linear Regression model\n",
        "linear_rmse = np.sqrt(mean_squared_error(y_test, linear_preds))\n",
        "print(f\"Linear Regression RMSE: {linear_rmse}\")"
      ],
      "metadata": {
        "id": "beHDqovAclFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task**:\n",
        "\n",
        "a. Perform hyperparameter tuning for the Ridge Regression model using GridSearchCV. Create a dictionary named `ridge_params` with the following keys: 'alpha' and 'solver'. The 'alpha' key should have a list of values: [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 1, 10, 100]. The 'solver' key should have a list of values: ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga'].\n",
        "\n",
        "b. Fit the GridSearchCV object with the scaled features (X_scaled) and target variable (y).\n",
        "\n",
        "c. Get the best Ridge Regression model from the GridSearchCV results and store it in a variable named `best_ridge`.\n",
        "\n",
        "d. Re-split the data using the scaled features (X_scaled) and target variable (y) with a test size of 0.2 and a random state of 42. Store the split data in the variables `X_train`, `X_test`, `y_train`, and `y_test`.\n",
        "\n",
        "e. Train the best Ridge Regression model on the training data (X_train and y_train).\n",
        "\n",
        "f. Make predictions on the testing data (X_test) and store the results in a variable named `ridge_preds`.\n",
        "\n",
        "g. Calculate the RMSE for the best Ridge Regression model using the true values (y_test) and the predicted values (ridge_preds). Store the result in a variable named `ridge_rmse` and print it."
      ],
      "metadata": {
        "id": "kPM_WzFPSUB_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform hyperparameter tuning for Ridge Regression\n",
        "# Perform hyperparameter tuning for Ridge Regression with additional parameters\n",
        "ridge_params = {\n",
        "    'alpha': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 1, 10, 100],\n",
        "    'solver': ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga']\n",
        "}\n",
        "grid_search.fit(X_scaled, y)\n",
        "\n",
        "# Get the best Ridge Regression model\n",
        "best_ridge = grid_search.best_estimator_\n",
        "\n",
        "# Re-split the data using the scaled features and updated dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the best Ridge Regression model on the training data\n",
        "# TODO - Write your solution here\n",
        "\n",
        "\n",
        "# Make predictions on the testing data\n",
        "ridge_preds = # TODO - Write your solution here\n",
        "\n",
        "# Calculate the RMSE for the best Ridge Regression model\n",
        "ridge_rmse = # TODO - Write your solution here\n",
        "print(f\"Best Ridge Regression RMSE: {ridge_rmse}\")"
      ],
      "metadata": {
        "id": "67etyaHFlnfJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}